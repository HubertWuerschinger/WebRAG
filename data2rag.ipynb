{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea8d90e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def convert_to_rag_format(crawled_jobs, output_file=\"rag_data.jsonl\"):\n",
    "    \"\"\"\n",
    "    Konvertiert gecrawlte Jobs in das JSONL-Format für ein RAG-System.\n",
    "\n",
    "    Args:\n",
    "        crawled_jobs (list): Liste der gecrawlten Stellenangebote.\n",
    "        output_file (str): Dateiname für die JSONL-Ausgabe.\n",
    "    \"\"\"\n",
    "    rag_data = []\n",
    "\n",
    "    for job in crawled_jobs:\n",
    "        # Titel und URL aus dem Job übernehmen\n",
    "        title = job.get(\"title\", \"Unbekannter Titel\")\n",
    "        url = job.get(\"url\", \"Keine URL\")\n",
    "        description = job.get(\"description\", \"Keine Beschreibung\")\n",
    "        \n",
    "        # Eintrag für RAG-System erstellen\n",
    "        entry = {\n",
    "            \"prompt\": f\"Was enthält die Seite '{title}'?\",\n",
    "            \"completion\": description,\n",
    "            \"meta\": {\n",
    "                \"title\": title,\n",
    "                \"url\": url,\n",
    "                \"timestamp\": datetime.utcnow().isoformat()\n",
    "            }\n",
    "        }\n",
    "        rag_data.append(entry)\n",
    "\n",
    "    # Daten in JSONL-Datei speichern\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        for entry in rag_data:\n",
    "            file.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"Daten erfolgreich in {output_file} gespeichert.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "740100e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reparierte Datei wurde in fixed_jobs_data.json gespeichert.\n"
     ]
    }
   ],
   "source": [
    "input_file = \"jobs_data.json\"\n",
    "output_file = \"fixed_jobs_data.json\"\n",
    "\n",
    "try:\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        raw_data = file.read()\n",
    "\n",
    "    # Ersetze einfache Anführungszeichen durch doppelte\n",
    "    fixed_data = raw_data.replace(\"'\", '\"')\n",
    "\n",
    "    # Reparierte Datei speichern\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(fixed_data)\n",
    "\n",
    "    print(f\"Reparierte Datei wurde in {output_file} gespeichert.\")\n",
    "except Exception as e:\n",
    "    print(f\"Fehler bei der Reparatur: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b143ca63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reparierte Datei erfolgreich gespeichert unter: repaired_jobs_data.json\n"
     ]
    }
   ],
   "source": [
    "input_file = \"jobs_data.json\"\n",
    "output_file = \"repaired_jobs_data.json\"\n",
    "\n",
    "try:\n",
    "    repaired_data = []\n",
    "    current_object = {}\n",
    "    \n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"{\"):\n",
    "                # Start eines neuen Objekts\n",
    "                current_object = {}\n",
    "            elif line.startswith(\"}\"):\n",
    "                # Ende eines Objekts -> Objekt speichern\n",
    "                repaired_data.append(current_object)\n",
    "            else:\n",
    "                # Versuche Schlüssel-Wert-Paare zu erkennen\n",
    "                if \":\" in line:\n",
    "                    key, value = line.split(\":\", 1)\n",
    "                    key = key.strip().strip('\"')  # Entfernt unnötige Leerzeichen/Anführungszeichen\n",
    "                    value = value.strip().strip(',').strip('\"')  # Entfernt Komma/Anführungszeichen\n",
    "                    current_object[key] = value\n",
    "    \n",
    "    # Reparierte Daten als gültige JSON-Liste speichern\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(repaired_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Reparierte Datei erfolgreich gespeichert unter: {output_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"Fehler bei der Reparatur: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d89f8851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 Jobs erfolgreich geladen.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "repaired_file = \"repaired_jobs_data.json\"\n",
    "\n",
    "with open(repaired_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    crawled_jobs = json.load(file)\n",
    "\n",
    "print(f\"{len(crawled_jobs)} Jobs erfolgreich geladen.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12d663fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daten erfolgreich in rag_data.jsonl gespeichert.\n",
      "Daten erfolgreich in rag_data.jsonl konvertiert.\n"
     ]
    }
   ],
   "source": [
    "output_file = \"rag_data.jsonl\"\n",
    "convert_to_rag_format(crawled_jobs, output_file=output_file)\n",
    "print(f\"Daten erfolgreich in {output_file} konvertiert.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d962fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0                          Technical Trainer (m/w/d)   \n",
      "1                        Cloud Engineer (AWS) Senior   \n",
      "2                      Senior Cloud Engineer (Azure)   \n",
      "3                           Cloud Engineer (AWS) Mid   \n",
      "4  Werkstudent (m/w/d) Sustainability / Corporate...   \n",
      "\n",
      "                         company  \\\n",
      "0   Körber Pharma Packaging GmbH   \n",
      "1  Körber Porto, Unipessoal Lda.   \n",
      "2  Körber Porto, Unipessoal Lda.   \n",
      "3  Körber Porto, Unipessoal Lda.   \n",
      "4                      Körber AG   \n",
      "\n",
      "                                            location  \\\n",
      "0  Schloß Holte-Stukenbrock, Nordrhein-Westfalen, DE   \n",
      "1                                          Porto, PT   \n",
      "2                                          Porto, PT   \n",
      "3                                          Porto, PT   \n",
      "4                                    Hamburg, HH, DE   \n",
      "\n",
      "                                                 url  \\\n",
      "0  https://jobs.koerber.com/pharma/job/Schlo%C3%9...   \n",
      "1  https://jobs.koerber.com/pharma/job/Porto-Clou...   \n",
      "2  https://jobs.koerber.com/pharma/job/Porto-Seni...   \n",
      "3  https://jobs.koerber.com/pharma/job/Porto-Clou...   \n",
      "4  https://jobs.koerber.com/job/Hamburg-Werkstude...   \n",
      "\n",
      "                                         description  \n",
      "0  Ihr Profil:\\nSie haben eine abgeschlossene Aus...  \n",
      "1  Your role in our team:\\nBS or MS in Computer S...  \n",
      "2  Your role in our team:\\nBe part of a diverse t...  \n",
      "3  Your role in our team:\\nBS or MS in Computer S...  \n",
      "4                                      Ihr Profil:\\n  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(crawled_jobs)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fd318f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON-Fehler in Zeile 1.\n",
      "Die Datei rag_data.jsonl ist ungültig.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def validate_jsonl(file_path):\n",
    "    \"\"\"\n",
    "    Validiert eine JSONL-Datei für die Verarbeitung zu Chunks und Vektorspeicher.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Pfad zur JSONL-Datei.\n",
    "\n",
    "    Returns:\n",
    "        bool: True, wenn die Datei gültig ist und verarbeitet werden kann.\n",
    "        list: Liste der validierten Datensätze.\n",
    "    \"\"\"\n",
    "    valid = True\n",
    "    records = []\n",
    "\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line_num, line in enumerate(f, start=1):\n",
    "                try:\n",
    "                    record = json.loads(line.strip())\n",
    "                    # Prüfen, ob notwendige Schlüssel vorhanden sind\n",
    "                    if not all(key in record for key in [\"prompt\", \"completion\", \"meta\"]):\n",
    "                        print(f\"Fehlender Schlüssel in Zeile {line_num}.\")\n",
    "                        valid = False\n",
    "                        continue\n",
    "                    \n",
    "                    # Prüfen, ob meta die Schlüssel 'title' und 'url' enthält\n",
    "                    if not all(key in record[\"meta\"] for key in [\"title\", \"url\"]):\n",
    "                        print(f\"Fehlender Metadaten-Schlüssel in Zeile {line_num}.\")\n",
    "                        valid = False\n",
    "                        continue\n",
    "                    \n",
    "                    # Prüfen, ob completion nicht leer ist\n",
    "                    if not record[\"completion\"].strip():\n",
    "                        print(f\"Leerer Textinhalt in Zeile {line_num}.\")\n",
    "                        valid = False\n",
    "                        continue\n",
    "\n",
    "                    records.append(record)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"JSON-Fehler in Zeile {line_num}.\")\n",
    "                    valid = False\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Datei {file_path} nicht gefunden.\")\n",
    "        return False, []\n",
    "\n",
    "    return valid, records\n",
    "\n",
    "\n",
    "# Beispielprüfung\n",
    "file_path = \"rag_data.jsonl\"\n",
    "is_valid, data = validate_jsonl(file_path)\n",
    "\n",
    "if is_valid:\n",
    "    print(f\"Die Datei {file_path} ist gültig und enthält {len(data)} Datensätze.\")\n",
    "else:\n",
    "    print(f\"Die Datei {file_path} ist ungültig.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "525c7161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_chunk_suitability(data, chunk_size=500):\n",
    "    \"\"\"\n",
    "    Prüft, ob die Datensätze groß genug sind, um in Chunks aufgeteilt zu werden.\n",
    "\n",
    "    Args:\n",
    "        data (list): Liste der Datensätze aus der JSONL-Datei.\n",
    "        chunk_size (int): Minimale Größe eines Chunks.\n",
    "\n",
    "    Returns:\n",
    "        bool: True, wenn die Datensätze geeignet sind.\n",
    "    \"\"\"\n",
    "    suitable = True\n",
    "    for idx, record in enumerate(data):\n",
    "        if len(record[\"completion\"]) < chunk_size:\n",
    "            print(f\"Datensatz {idx + 1} ist zu kurz für Chunks (nur {len(record['completion'])} Zeichen).\")\n",
    "            suitable = False\n",
    "    return suitable\n",
    "\n",
    "\n",
    "if is_valid:\n",
    "    if check_chunk_suitability(data):\n",
    "        print(\"Alle Datensätze sind groß genug für Chunking.\")\n",
    "    else:\n",
    "        print(\"Einige Datensätze sind zu klein für Chunking.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3dfe35a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON-Fehler in Zeile 1: Invalid \\escape: line 1 column 261 (char 260)\n",
      "Validierte 249 Datensätze erfolgreich.\n"
     ]
    }
   ],
   "source": [
    "def validate_and_fix_jsonl(file_path):\n",
    "    \"\"\"\n",
    "    Validiert eine JSONL-Datei Zeile für Zeile und korrigiert fehlerhafte JSON-Einträge, falls möglich.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Pfad zur JSONL-Datei.\n",
    "\n",
    "    Returns:\n",
    "        list: Liste der validen Datensätze.\n",
    "    \"\"\"\n",
    "    valid_records = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line_num, line in enumerate(f, start=1):\n",
    "            try:\n",
    "                record = json.loads(line.strip())\n",
    "                valid_records.append(record)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSON-Fehler in Zeile {line_num}: {e}\")\n",
    "                # Optional: Protokolliere die fehlerhaften Zeilen oder versuche eine Korrektur\n",
    "    return valid_records\n",
    "\n",
    "# Beispiel: Validierung\n",
    "file_path = \"rag_data.jsonl\"\n",
    "valid_data = validate_and_fix_jsonl(file_path)\n",
    "\n",
    "if valid_data:\n",
    "    print(f\"Validierte {len(valid_data)} Datensätze erfolgreich.\")\n",
    "else:\n",
    "    print(\"Keine gültigen Datensätze gefunden.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90f0a0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fehlerhafte Zeile 1 übersprungen.\n",
      "Reformatierung abgeschlossen. 249 Zeilen erfolgreich gespeichert.\n"
     ]
    }
   ],
   "source": [
    "def reformat_jsonl(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Reformatiert eine JSONL-Datei, indem fehlerhafte Zeilen übersprungen werden.\n",
    "\n",
    "    Args:\n",
    "        input_path (str): Pfad zur Eingabedatei.\n",
    "        output_path (str): Pfad zur Ausgabedatei.\n",
    "\n",
    "    Returns:\n",
    "        int: Anzahl der validierten Zeilen.\n",
    "    \"\"\"\n",
    "    valid_count = 0\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as infile, open(output_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        for line_num, line in enumerate(infile, start=1):\n",
    "            try:\n",
    "                record = json.loads(line.strip())\n",
    "                json.dump(record, outfile, ensure_ascii=False)\n",
    "                outfile.write(\"\\n\")\n",
    "                valid_count += 1\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Fehlerhafte Zeile {line_num} übersprungen.\")\n",
    "    return valid_count\n",
    "\n",
    "# Neuformatieren der JSONL-Datei\n",
    "input_path = \"rag_data.jsonl\"\n",
    "output_path = \"cleaned_rag_data.jsonl\"\n",
    "valid_lines = reformat_jsonl(input_path, output_path)\n",
    "\n",
    "print(f\"Reformatierung abgeschlossen. {valid_lines} Zeilen erfolgreich gespeichert.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1a05ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
